{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aMWnmv0lXHO"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CristianR8/Imagenes-Espectrales-Sesion4-Parte1-HoCV/main/images/banner-spectral.png\" width=\"1000\" align=\"middle\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLRNRbhzzxUu"
      },
      "source": [
        "\n",
        "# <font color='#ECA702'>**Hands-on Sesión 4: Clasificacion espectral 📸 🌈**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzOWtyo3uf7"
      },
      "source": [
        "# <font color='#4C5FDA'>**Objetivo**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcaIYiXr39TU"
      },
      "source": [
        "El objetivo de la clasificación hiperespectral es clasificar cada píxel/punto de datos en una de $K$ clases.  En general, los métodos de clasificación son más eficaces que los de desmezcla hiperespectral. Sin embargo, los métodos de clasificación no son eficaces a la hora de determinar las cantidades de proporción subpíxel o la cantidad de un material que puede encontrarse dentro del campo de visión correspondiente a un píxel.  \n",
        "\n",
        "En general, los enfoques de clasificación hiperespectral implican:\n",
        "1. (opcionalmente) extracción de características\n",
        "2. aplicación de un clasificador estándar (es decir, clasificadores de la bibliografía sobre aprendizaje automático).\n",
        "\n",
        "Para esta sesión haremos uso de 3 clasificadores comunes segun el estado del arte en la clasificacion espectral de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6CXO95ZyjkX"
      },
      "source": [
        "# <font color='#4C5FDA'>**Explicación métricas empleadas**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBmD07AFyjkZ"
      },
      "source": [
        "- Precisión (Accuracy): La precisión es probablemente la métrica más directa y sencilla para entender. Imagina que tienes un conjunto de imágenes y un clasificador que intenta identificar si cada imagen contiene o no un perro. Si el clasificador evalúa 100 imágenes y acierta (correctamente identifica si hay o no un perro) en 90 de ellas, entonces la precisión del clasificador es del 90%. En términos matemáticos, la precisión se calcula como el número de predicciones correctas (tanto positivas como negativas) dividido por el número total de predicciones hechas. Se puede expresar como:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/CristianR8/Imagenes-Espectrales-Sesion4-Parte1-HoCV/main/images/accuracy.png\" alt=\"Imagenes espectrales\" style=\"width: 400px;\"/>\n",
        "</div>\n",
        "\n",
        "- Índice de Jaccard: El índice de Jaccard, también conocido como la Intersección sobre la Unión (IoU), es una medida un poco más sofisticada que se usa especialmente para evaluar la calidad de los clasificadores en tareas de segmentación de imágenes, donde no solo importa saber si una imagen contiene un objeto específico, sino también dónde está ese objeto dentro de la imagen.  Se calcula como la intersección (el área que ambos rectángulos, el predicho y el verdadero, comparten) dividida por la unión (el área total cubierta por ambos rectángulos, sin contar dos veces las áreas que se superponen). Esto se expresa matemáticamente como:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/CristianR8/Imagenes-Espectrales-Sesion4-Parte1-HoCV/main/images/jaccard.webp\" alt=\"Imagenes espectrales\" style=\"width: 400px;\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUkJ3TPRyjka"
      },
      "source": [
        "**Al finalizar este Notebook deberas obtener una grafica comparativa con las predicciones de los tres clasificadores propuestos**\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/CristianR8/Imagenes-Espectrales-Sesion4-Parte1-HoCV/main/images/objetivo.png\" alt=\"Imagenes espectrales\" style=\"width: 700px;\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU0q6r7myjkb"
      },
      "source": [
        "# <font color='#4C5FDA'>**Importamos datos necesarios**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQou0Fz0yjkc"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "%%capture\n",
        "!pip install gdown\n",
        "!gdown --id 1ob4CwfeG-g2PWaxJdnmFnF4gB0e9Dd8d #PaviaU_gt.mat\n",
        "!gdown --id 1ZjpMKaMTSLbM4x3XpMCpootVjkLiFWde #PaviaU.mat\n",
        "!gdown --id 13X3I26JniCKag4DTrXF6d8f3xBs2o7Td #QPP.py\n",
        "!gdown --id 14BTtiRg0BEP30PBULLUQ-NWG3ydmsyyH #SPICE.py\n",
        "!gdown --id 1XP2gip7PFl04ojFoAsWaJ9hdeCFXfU9Y #cnn_x_test.pth\n",
        "!gdown --id 14kwBxZOU1ttYacU0P37bgRK8cXcM7tOB #cnn_x_train.pth\n",
        "!gdown --id 1zXFhgjAbN38P1sZxOTTEnpaj72ZKQmx4 #cnn_y_test.pth\n",
        "!gdown --id 14rAst2xEdoAeT5M-JZOGrlx2piAjmV67 #cnn_y_train.pth\n",
        "!gdown --id 1gxVjW0v-1e11-iGUfQHdmpjvkWtb4nie #model_pca.pkl\n",
        "!gdown --id 1vXEziBRgej4-PFYn5b5aQhMchLZZtOdb #complete_model.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihjoJFVnFtHm"
      },
      "source": [
        "# <font color='#4C5FDA'>**Importamos librerias**</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ZjpMKaMTSLbM4x3XpMCpootVjkLiFWde #PaviaU.mat"
      ],
      "metadata": {
        "id": "qoVAfGZvyW6A",
        "outputId": "ed725b6f-0d08-440a-ba0e-a16948082b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1ZjpMKaMTSLbM4x3XpMCpootVjkLiFWde\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-UV64zD5pGdU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install earthpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0f-uPpbmFpO2"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "import earthpy.spatial as es\n",
        "import earthpy.plot as epp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, jaccard_score\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "\n",
        "sns.set()\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzR77eFUecxL"
      },
      "source": [
        "## <font color='#4C5FDA'>**Información del Dataset**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rPiTRY4eZAV"
      },
      "source": [
        "El conjunto de datos de la Universidad de Pavía es un conjunto de datos de imágenes hiperespectrales recogidas por un sensor conocido como espectrómetro de imágenes de sistema óptico reflectante (ROSIS-3) sobre la ciudad de Pavía, Italia. La imagen consta de 610×340 píxeles con 115 bandas espectrales. La imagen se divide en 9 clases con un total de 42.776 muestras etiquetadas, entre las que se incluyen el asfalto, los prados, la grava, los árboles, la chapa metálica, el suelo desnudo, el betún, el ladrillo y la sombra.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/CristianR8/Imagenes-Espectrales-Sesion4-Parte1-HoCV/main/images/paviau.jpg\" alt=\"Imagenes espectrales\" style=\"width: 600px;\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW-91xCJeM7y"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Cargamos el conjunto de datos**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "_pYyByS52xTO",
        "outputId": "82aa6796-6ecb-493a-f3ae-28c4df4023b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'PaviaU.mat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PaviaU.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ba4108e51807>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PaviaU.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mground_truth_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PaviaU_gt.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_paviau_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Visualizar la primera banda de la imagen y la verdad fundamental para verificar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ba4108e51807>\u001b[0m in \u001b[0;36mload_paviau_dataset\u001b[0;34m(data_path, ground_truth_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_paviau_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m    232\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise OSError(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PaviaU.mat'"
          ]
        }
      ],
      "source": [
        "def load_paviau_dataset(data_path, ground_truth_path):\n",
        "\n",
        "    data = loadmat(data_path)\n",
        "    gt = loadmat(ground_truth_path)\n",
        "\n",
        "    # Assuming the variable names in the .mat files are 'paviaU' and 'paviaU_gt' respectively.\n",
        "    # Adjust the keys if they are different in your dataset files.\n",
        "    img = data['paviaU']\n",
        "    gt = gt['paviaU_gt']\n",
        "\n",
        "    return img, gt\n",
        "\n",
        "# Funcion de preprocesamiento\n",
        "def preprocess_data(img, gt):\n",
        "    n_rows, n_cols, n_bands = img.shape\n",
        "    pixels = img.reshape((n_rows*n_cols, n_bands))\n",
        "    labels = gt.ravel()\n",
        "    return pixels, labels\n",
        "\n",
        "# Ejemplo de uso\n",
        "data_path = 'PaviaU.mat'\n",
        "ground_truth_path = 'PaviaU_gt.mat'\n",
        "img, gt = load_paviau_dataset(data_path, ground_truth_path)\n",
        "\n",
        "# Visualizar la primera banda de la imagen y la verdad fundamental para verificar\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img[:, :, 0], cmap='gray')\n",
        "plt.title('First Band of Hyperspectral Image')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground Truth Labels')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfV5Rx01QMhZ"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Preparación de los datos para el clasificador KNN**</small></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRKkTrBXrrdw"
      },
      "source": [
        "\n",
        "Ahora podemos importar nuestro conjunto de datos de imágenes aéreas y convertirlo en un formato tabular para facilitar las operaciones de procesamiento sobre él. En este caso, cada banda de imagen se convierte en una columna **(¡tenemos más de 100 bandas!)**, y se crea una columna de clase para almacenar los datos sobre nuestras etiquetas, con cada objeto clasificado posible representado como un número (en total 10). Luego se eliminan los elementos asociados con la clase 0, ya que la clase 0 se ha utilizado como una categoría general para todos los objetos no clasificados en la imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkb1JV_d3Q18",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "n_rows, n_cols, n_bands = img.shape  # Obtiene las dimensiones de la imagen: filas, columnas y bandas espectrales\n",
        "pixels = img.reshape((n_rows*n_cols, n_bands))  # Reorganiza la imagen en una matriz de píxeles (cada píxel con sus bandas espectrales)\n",
        "labels = gt.ravel()  # Aplana el array de etiquetas del terreno para que coincida con la estructura de los píxeles\n",
        "pixels, labels  # Muestra los arrays de píxeles y etiquetas\n",
        "\n",
        "pixels, labels = preprocess_data(img, gt)  # Preprocesa los datos de la imagen y las etiquetas del terreno\n",
        "# Filtra los píxeles que no tienen etiqueta en el terreno (etiquetas = 0)\n",
        "pixels = pixels[labels > 0]  # Selecciona solo los píxeles con etiquetas de terreno\n",
        "labels = labels[labels > 0]  # Selecciona solo las etiquetas correspondientes a esos píxeles\n",
        "\n",
        "# Divide el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(pixels, labels, test_size=0.3, random_state=42)  # Usa el 30% de los datos para prueba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtzY30YoQwL-"
      },
      "source": [
        "# <font color='#ECA702'>**<font color=\"#FF0000\">R</font><font color=\"#FF7F00\">e</font><font color=\"#FFFF00\">t</font><font color=\"#00FF00\">o</font> #<font color=\"#0000FF\">1</font>** 💪</font>\n",
        "* Construye el clasificador KNN, ayudate de la documentacion oficial en el siguiente enlace: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVGBL4l0yjkl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Entrenar clasificador KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "# Predecir en el conjunto de pruebas\n",
        "y_pred = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMt3aD_wyjkl"
      },
      "outputs": [],
      "source": [
        "# Evaluacion\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Indice Jaccard: \", jaccard_score(y_test, y_pred, average='macro') )\n",
        "print(\"Reporte de clasificacion:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Matriz de Confusion:\")\n",
        "cf_matrix_knn = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix_knn, annot=True, cmap=sns.cubehelix_palette(as_cmap=True), fmt='g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntrjkfFcyjkl"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Entrenamiento del clasificador KNN usando todos los datos**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcIA-6uOz7px"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "# Clasificar toda la imagen para su visualización\n",
        "full_img_prediction = knn.predict(img.reshape((-1, img.shape[2])))\n",
        "pred_knn = full_img_prediction.reshape((img.shape[0], img.shape[1]))\n",
        "gt_flat = gt.ravel()\n",
        "mask = gt_flat != 0\n",
        "gt_filtered = gt_flat[mask]\n",
        "full_img_prediction_filtered = full_img_prediction[mask]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(gt_filtered, full_img_prediction_filtered))\n",
        "print(\"Reporte de clasificacion:\")\n",
        "print(classification_report(gt_flat, full_img_prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yum5t43Wyjkm"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Graficar la prediccion del clasificador KNN**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vTC42Ky6fMX"
      },
      "outputs": [],
      "source": [
        "# Trazar la imagen clasificada\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(pred_knn, cmap='jet')\n",
        "plt.title('Imagen clasificada con KNN')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground Truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGg7jQcRe9CN"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Preprocesamiento para el clasificador Decision Tree**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMUVHE46UKld"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "\n",
        "df = pd.DataFrame(img.reshape(img.shape[0]*img.shape[1], -1))\n",
        "df.columns = [f'band{i}' for i in range(1, df.shape[-1]+1)]\n",
        "df['class'] = gt.ravel()\n",
        "df = df[df['class']!=0]\n",
        "\n",
        "stacked_bands = np.transpose(img, (2, 0, 1))\n",
        "sampled_bands = np.array([stacked_bands[0], stacked_bands[50], stacked_bands[100]])\n",
        "bands = [f'Band {i}' for i in range(1, 102, 50)]\n",
        "colors = list(mcolors.BASE_COLORS)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "epp.plot_rgb(\n",
        "    stacked_bands,\n",
        "    rgb=(60, 30, 27),\n",
        "    stretch=True,\n",
        "    figsize=(10, 10),\n",
        "    ax=plt.gca(),\n",
        ")\n",
        "plt.title('Imagen convertida')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MyCigM-fQjw"
      },
      "source": [
        "# <font color='#ECA702'>**<font color=\"#FF0000\">R</font><font color=\"#FF7F00\">e</font><font color=\"#FFFF00\">t</font><font color=\"#00FF00\">o</font> #<font color=\"#0000FF\">2</font>** 💪</font>\n",
        "* Construye el clasificador Decision Tree, ayudate de la documentacion oficial en el siguiente enlace: https://scikit-learn.org/stable/modules/tree.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCfXRMc-yjko"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "x = df.drop(['class'], axis=1)\n",
        "y = df['class']\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, stratify = y)\n",
        "y_encoder = le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab9Ynhp0yjko",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "model_dt = dt.fit(x_train.values, y_train)\n",
        "y_pred = model_dt.predict(x_test.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uuYUjokyjkp"
      },
      "outputs": [],
      "source": [
        "# Evaluacion\n",
        "print(\"Accuracy Score: \", round(accuracy_score(y_test, y_pred), 3)*100, \"%\")\n",
        "print(\"Indice Jaccard: \", jaccard_score(y_test, y_pred, average='macro'))\n",
        "print(\"Reporte de clasificacion:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "cf_matrix_dt = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cf_matrix_dt, annot=True, cmap=sns.cubehelix_palette(as_cmap=True), fmt='g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_fYLleRyjkp"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Entrenamiento del clasificador Decision Tree**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxd_IykFEmyy"
      },
      "outputs": [],
      "source": [
        "l = []\n",
        "for i in range(img.shape[0]*img.shape[1]):\n",
        "    if i in list(df.index):\n",
        "        l.append(le.inverse_transform(model_dt.predict([df.loc[i, :][:-1]])))\n",
        "    else:\n",
        "        l.append(0)\n",
        "\n",
        "pred_dt = np.array(l, dtype=object).reshape(gt.shape).astype('float')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jcAd6ecyjkq"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Graficar la prediccion del clasificador Decision Tree**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzvBrPI1MX3z"
      },
      "outputs": [],
      "source": [
        "# Trazar la imagen clasificada\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(pred_dt, cmap='jet')\n",
        "plt.title('Imagen clasificada con Decision Tree')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground Truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHufrtXaeUZ1"
      },
      "source": [
        "# <font color='#ECA702'>**<font color=\"#FF0000\">R</font><font color=\"#FF7F00\">e</font><font color=\"#FFFF00\">t</font><font color=\"#00FF00\">o</font> #<font color=\"#0000FF\">3</font>** 💪</font>\n",
        "* Evalúa el modelo de redes neuronales que implementamos para que visualices los resultados de esta clasificación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JUZo0eqyjkq"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Declaracion del modelo**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQNAqBCQD9_P"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.cv1 = nn.Conv3d(1, 8, kernel_size=(3,3, 5))\n",
        "        self.cv2 = nn.Conv2d(8, 16, kernel_size=(3,3))\n",
        "        self.fc1 = nn.Linear(100048, 128)\n",
        "        self.dp = nn.Dropout(p=0.4)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.cv1(x)\n",
        "        out = F.relu(out)\n",
        "        out = torch.reshape(out, (out.shape[0], out.shape[1], out.shape[2], out.shape[3]*out.shape[4]))\n",
        "        out = self.cv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.dp(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVOnbIheGbbB"
      },
      "outputs": [],
      "source": [
        "def zeros_pad(x, margin):\n",
        "    padded_x = torch.zeros((x.shape[0] + 2 * margin, x.shape[1] + 2 * margin, x.shape[2]))\n",
        "    padded_x[margin:x.shape[0] + margin, margin:x.shape[1] + margin, :] = x\n",
        "    return padded_x\n",
        "\n",
        "def create_image(x, y, window_size):\n",
        "    margin = (window_size - 1) // 2\n",
        "    padded_x = zeros_pad(x, margin=margin)\n",
        "    patched_x = torch.zeros((x.shape[0] * x.shape[1], window_size, window_size, x.shape[2]))\n",
        "    patched_y = torch.zeros((x.shape[0] * x.shape[1]))\n",
        "    patch_index = 0\n",
        "    for i in range(margin, padded_x.shape[0] - margin):\n",
        "        for j in range(margin, padded_x.shape[1] - margin):\n",
        "            patch = padded_x[i - margin:i + margin + 1, j - margin:j + margin + 1]\n",
        "            patched_x[patch_index, :, :, :] = patch\n",
        "            patched_y[patch_index] = y[i-margin, j-margin]\n",
        "            patch_index += 1\n",
        "    patched_x = patched_x[patched_y>0,:,:,:]\n",
        "    patched_y = patched_y[patched_y>0]\n",
        "    patched_y -= 1\n",
        "    return patched_x, patched_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXaJI5Myyjk4"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Cargamos los modelos necesarios**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzPQE9u3Ce7S"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "\n",
        "# Cargar el modelo de Deep Learning completo\n",
        "model = torch.load('complete_model.pth', weights_only=False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47X1RFA-EI5f"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "\n",
        "# Cargar el modelo PCA\n",
        "pca = joblib.load('model_pca.pkl')\n",
        "\n",
        "# Cargar los conjuntos de datos particionados\n",
        "cnn_x_train = torch.load('cnn_x_train.pth')\n",
        "cnn_x_test = torch.load('cnn_x_test.pth')\n",
        "cnn_y_train = torch.load('cnn_y_train.pth')\n",
        "cnn_y_test = torch.load('cnn_y_test.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llyRmDCsGCjh"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "\n",
        "dimensions = 17\n",
        "window_size = 25\n",
        "test_perc = 0.3\n",
        "\n",
        "# Aquí realizamos la creación de DataLoader para la evaluación\n",
        "# No necesitas aplicar PCA de nuevo, ya que los datos ya están transformados y listos para usarse\n",
        "test = torch.utils.data.TensorDataset(cnn_x_test, cnn_y_test)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mguWx3dyjk6"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Ejecuta las siguientes 2 celdas para evaluar el modelo pre-entrenado**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YZxzdGro1QA"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "def confusion_matrix(data, nb_classes):\n",
        "    df_cm = pd.DataFrame(data,\n",
        "                          range(nb_classes), range(nb_classes))\n",
        "    plt.figure(figsize=(10,7))\n",
        "    sns.set(font_scale=1.4) # for label size\n",
        "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap='Blues',\n",
        "                fmt='g')\n",
        "    plt.title(\"Confusion Matrix\", fontsize = 20)\n",
        "    plt.xlabel(\"Predicted Output\", fontsize = 18)\n",
        "    plt.ylabel(\"Expected Output\", fontsize = 18)\n",
        "    plt.show()\n",
        "\n",
        "def acc_per_class(model, testloader, nb_classes):\n",
        "    model.eval()\n",
        "    confusion_mat = torch.zeros(nb_classes, nb_classes)\n",
        "    class_correct = torch.zeros(10)\n",
        "    class_total = torch.zeros(10)\n",
        "    total = 0\n",
        "    for inputs, labels in testloader:\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "            confusion_mat[t.long(), p.long()] += 1\n",
        "\n",
        "    confusion_matrix(confusion_mat.data.cpu().numpy(), nb_classes)\n",
        "\n",
        "    per_class_acc = 100*confusion_mat.diag()/confusion_mat.sum(1)\n",
        "    for i, j in enumerate(per_class_acc.data.cpu().numpy()):\n",
        "        print(\"Class:\", i, \"Accuracy:\", j)\n",
        "    acc = torch.mean(per_class_acc).data.cpu().numpy()\n",
        "    print(\"Overall Accuracy: \", acc, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fppjd0fuo24X"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "x = np.reshape(img, (-1, img.shape[2]))\n",
        "x_pca = pca.fit_transform(x)\n",
        "x_pca = np.reshape(x_pca, (img.shape[0], img.shape[1], dimensions))\n",
        "padded_x = zeros_pad(torch.tensor(x_pca), window_size//2)\n",
        "\n",
        "pred = np.zeros((gt.shape[0], gt.shape[1]))\n",
        "for h in range(gt.shape[0]):\n",
        "    for w in range(gt.shape[1]):\n",
        "        if int(gt[h, w]) == 0:\n",
        "            continue\n",
        "        else:\n",
        "            model.eval()\n",
        "            image_patch = padded_x[h:h+window_size, w:w+window_size, :]\n",
        "            image = torch.permute(image_patch[None, None, :, :, :], (0, 1, 4, 2, 3))\n",
        "            pred[h][w] = model(image).argmax(dim=1) + 1\n",
        "\n",
        "acc_per_class(model, testloader, 9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R8wSRSUyjk7"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Ejecuta la siguiente celda para obtener el Índice de Jaccard**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWKxj31Fc5g-"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "def jaccard_coefficient(pred, gt, nb_classes):\n",
        "    jaccard_per_class = np.zeros(nb_classes)\n",
        "    for cls in range(1, nb_classes + 1):  # Class labels start from 1\n",
        "        pred_cls = pred == cls\n",
        "        gt_cls = gt == cls\n",
        "        intersection = np.logical_and(pred_cls, gt_cls).sum()\n",
        "        union = np.logical_or(pred_cls, gt_cls).sum()\n",
        "        if union == 0:\n",
        "            jaccard_per_class[cls - 1] = np.nan  # To handle division by zero\n",
        "        else:\n",
        "            jaccard_per_class[cls - 1] = intersection / union\n",
        "    return np.nanmean(jaccard_per_class), jaccard_per_class  # Return average and per-class Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2OECBWMKI_j"
      },
      "outputs": [],
      "source": [
        "# NO MODIFIQUES ESTA CELDA\n",
        "nb_classes = 9  # Número de clases\n",
        "avg_jaccard, jaccard_per_class = jaccard_coefficient(pred, gt, nb_classes)\n",
        "print(\"Average Jaccard Coefficient:\", avg_jaccard)\n",
        "print(\"Jaccard Coefficient per Class:\", jaccard_per_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDPRYvKnyjk8"
      },
      "source": [
        "# <font color='#4C5FDA'><small>**Graficar la prediccion del clasificador con Deep Learning**</small></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWO4LB4b53wg"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(pred, cmap='jet')\n",
        "plt.title('Imagen clasificada con Deep Learning')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground Truth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwF7sTjO6ICn"
      },
      "source": [
        "# **<font color=\"#FF0000\">E</font><font color=\"#FF7F00\">x</font><font color=\"#FFFF00\">t</font><font color=\"#00FF00\">r</font><font color=\"#0000FF\">a</font>**: Grafica los tres resultados de los tres clasificadores y compara los resultados de manera visual. ¿Cual crees que es el mejor clasificador y por qué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sE0Fko7B6knD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "df934c30-c701-4bee-ccde-4ae9b568b468"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pred_knn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-435a54ea14db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_knn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KNN Classification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_knn' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(pred_knn, cmap='jet')\n",
        "plt.title('KNN Classification')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(pred_dt, cmap='jet')\n",
        "plt.title('Decision Tree Classification')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(pred, cmap='jet')\n",
        "plt.title('Deep Learning Classification')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(gt, cmap='jet')\n",
        "plt.title('Ground Truth')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}